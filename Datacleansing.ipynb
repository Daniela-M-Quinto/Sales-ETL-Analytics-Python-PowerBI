{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db649897",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# HU2 — Data Cleaning and Normalization\n",
    "\n",
    "This notebook demonstrates the cleaning and normalization process of the sales dataset (`sales`).  \n",
    "Steps include:\n",
    "\n",
    "1. **Load Data:** Load data from PostgreSQL.\n",
    "2. **Initial Quality Check:** Review missing values, duplicates, data types, and statistics.\n",
    "3. **Data Cleaning:** Remove duplicates, fill missing values, standardize text and column names, normalize data types.\n",
    "4. **Post-Cleaning Validation:** Verify completeness, duplicates, and retention rate.\n",
    "5. **Data Quality Visualizations:** Generate plots to visualize missing data, record retention, and completeness.\n",
    "6. **Summary Table:** Create a summary table with column-level quality metrics.\n",
    "7. **Save Clean Data:** Export cleaned dataset to CSV.\n",
    "8. **Upload to PostgreSQL:** Save the cleaned dataset to the database.\n",
    "9. **Final Report:** Summarize acceptance criteria and generated files.\n",
    "\n",
    "Author: Daniela \n",
    "Date: 28/11/2024  \n",
    "Database: RiwiVentas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab2622b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Data loaded successfully\n",
      "Total records: 1,250,000\n",
      "Total columns: 11\n",
      "Columns: ['city', 'date', 'product', 'product_type', 'quantity', 'unit_price', 'sale_type', 'customer_type', 'discount', 'shipping_cost', 'total_sales']\n",
      "\n",
      "Initial Data Quality Check\n",
      "------------------------------------------------------------\n",
      "DataFrame shape: 1,250,000 rows × 11 columns\n",
      "Memory usage: 764421.17 KB\n",
      "\n",
      "Data types:\n",
      "  • city: object\n",
      "  • date: object\n",
      "  • product: object\n",
      "  • product_type: object\n",
      "  • quantity: object\n",
      "  • unit_price: object\n",
      "  • sale_type: object\n",
      "  • customer_type: object\n",
      "  • discount: object\n",
      "  • shipping_cost: object\n",
      "  • total_sales: object\n",
      "\n",
      "Missing values per column:\n",
      "Column                    |  Missing |      %\n",
      "---------------------------------------------\n",
      "city                      |     1142 |   0.09%\n",
      "date                      |     1139 |   0.09%\n",
      "product                   |     1149 |   0.09%\n",
      "product_type              |     1103 |   0.09%\n",
      "quantity                  |     1163 |   0.09%\n",
      "unit_price                |     1149 |   0.09%\n",
      "sale_type                 |     1176 |   0.09%\n",
      "customer_type             |     1131 |   0.09%\n",
      "discount                  |     1083 |   0.09%\n",
      "shipping_cost             |     1088 |   0.09%\n",
      "total_sales               |     1165 |   0.09%\n",
      "\n",
      "Total duplicate rows: 4068\n",
      "\n",
      "Descriptive statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>date</th>\n",
       "      <th>product</th>\n",
       "      <th>product_type</th>\n",
       "      <th>quantity</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>sale_type</th>\n",
       "      <th>customer_type</th>\n",
       "      <th>discount</th>\n",
       "      <th>shipping_cost</th>\n",
       "      <th>total_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1248858</td>\n",
       "      <td>1248861</td>\n",
       "      <td>1248851</td>\n",
       "      <td>1248897</td>\n",
       "      <td>1248837</td>\n",
       "      <td>1248851</td>\n",
       "      <td>1248824</td>\n",
       "      <td>1248869</td>\n",
       "      <td>1248917</td>\n",
       "      <td>1248912</td>\n",
       "      <td>1248835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>188</td>\n",
       "      <td>146</td>\n",
       "      <td>72</td>\n",
       "      <td>36</td>\n",
       "      <td>34</td>\n",
       "      <td>5185</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>50009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Trujillo</td>\n",
       "      <td>2025-10-31</td>\n",
       "      <td>Café</td>\n",
       "      <td>Abarrotes</td>\n",
       "      <td>10.0</td>\n",
       "      <td>???</td>\n",
       "      <td>Distribuidor</td>\n",
       "      <td>Gobierno</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>???</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>44849</td>\n",
       "      <td>42062</td>\n",
       "      <td>104476</td>\n",
       "      <td>208459</td>\n",
       "      <td>125348</td>\n",
       "      <td>853</td>\n",
       "      <td>312611</td>\n",
       "      <td>312302</td>\n",
       "      <td>250412</td>\n",
       "      <td>831378</td>\n",
       "      <td>812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            city        date  product product_type quantity unit_price  \\\n",
       "count    1248858     1248861  1248851      1248897  1248837    1248851   \n",
       "unique       188         146       72           36       34       5185   \n",
       "top     Trujillo  2025-10-31     Café    Abarrotes     10.0        ???   \n",
       "freq       44849       42062   104476       208459   125348        853   \n",
       "\n",
       "           sale_type customer_type discount shipping_cost total_sales  \n",
       "count        1248824       1248869  1248917       1248912     1248835  \n",
       "unique            24            24        6             9       50009  \n",
       "top     Distribuidor      Gobierno      0.0           0.0         ???  \n",
       "freq          312611        312302   250412        831378         812  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed: 4068\n",
      "\n",
      "Post-Cleaning Validation\n",
      "Initial records: 1,250,000\n",
      "Final records:   1,245,932\n",
      "Records removed: 4,068\n",
      "Completeness:    100.00%\n",
      "Remaining duplicates: 16\n",
      "Remaining missing values: 0\n",
      "\n",
      "Clean data saved as 'ventas_limpias.csv'\n",
      "Records in PostgreSQL: 1,245,932\n",
      "\n",
      "HU2 COMPLETED - Cleaned data ready for analysis\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Import Libraries\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. Load Data from PostgreSQL\n",
    "# ============================================================\n",
    "DATABASE_URL = \"postgresql+psycopg2://postgres:123@localhost:5432/Riwiventas\"\n",
    "\n",
    "try:\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    df = pd.read_sql(\"SELECT * FROM sales\", engine)\n",
    "    df_initial = df.copy()  # Save copy for comparison\n",
    "    initial_records = len(df)\n",
    "    \n",
    "    print(f\"Data loaded successfully\")\n",
    "    print(f\"Total records: {initial_records:,}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ============================================================\n",
    "# 3. Initial Quality Diagnosis\n",
    "# ============================================================\n",
    "print(\"\\nInitial Data Quality Check\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "for col, dtype in df.dtypes.items():\n",
    "    print(f\"  • {col}: {dtype}\")\n",
    "\n",
    "# Missing values\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    print(f\"{'Column':<25} | {'Missing':>8} | {'%':>6}\")\n",
    "    print(\"-\"*45)\n",
    "    for col in df.columns:\n",
    "        count = missing[col]\n",
    "        pct = (count / len(df)) * 100\n",
    "        if count > 0:\n",
    "            print(f\"{col:<25} | {count:>8} | {pct:>6.2f}%\")\n",
    "else:\n",
    "    print(\"No missing values detected.\")\n",
    "\n",
    "# Duplicates\n",
    "total_duplicates = df.duplicated().sum()\n",
    "print(f\"\\nTotal duplicate rows: {total_duplicates}\")\n",
    "\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "display(df.describe())\n",
    "\n",
    "# ============================================================\n",
    "# 4. Data Cleaning and Transformation\n",
    "# ============================================================\n",
    "\n",
    "# 4.1 Remove exact duplicates\n",
    "df = df.drop_duplicates()\n",
    "removed_duplicates = initial_records - len(df)\n",
    "print(f\"Duplicates removed: {removed_duplicates}\")\n",
    "\n",
    "# 4.2 Fill missing values\n",
    "for col in df.columns:\n",
    "    n_missing = df[col].isnull().sum()\n",
    "    if n_missing > 0:\n",
    "        if df[col].dtype in ['float64','int64']:\n",
    "            df[col].fillna(df[col].mean(), inplace=True)\n",
    "        else:\n",
    "            df[col].fillna('unknown', inplace=True)\n",
    "\n",
    "# 4.3 Normalize column names\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# 4.4 Strip whitespace in text columns\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col] = df[col].str.strip()\n",
    "\n",
    "# 4.5 Convert text to lowercase\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col] = df[col].str.lower()\n",
    "\n",
    "# 4.6 Normalize data types\n",
    "for col in df.columns:\n",
    "    if col.lower() in ['fecha','date','created','updated','date_time','fecha_venta']:\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# ============================================================\n",
    "# 5. Post-Cleaning Validation\n",
    "# ============================================================\n",
    "final_records = len(df)\n",
    "completeness = df.notna().sum().sum() / (final_records * len(df.columns)) * 100\n",
    "\n",
    "print(\"\\nPost-Cleaning Validation\")\n",
    "print(f\"Initial records: {initial_records:,}\")\n",
    "print(f\"Final records:   {final_records:,}\")\n",
    "print(f\"Records removed: {initial_records - final_records:,}\")\n",
    "print(f\"Completeness:    {completeness:.2f}%\")\n",
    "print(f\"Remaining duplicates: {df.duplicated().sum()}\")\n",
    "print(f\"Remaining missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# ============================================================\n",
    "# 6. Save Clean Data to CSV\n",
    "# ============================================================\n",
    "clean_file = \"ventas_limpias.csv\"\n",
    "df.to_csv(clean_file, index=False, encoding='utf-8')\n",
    "print(f\"\\nClean data saved as '{clean_file}'\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. Upload Clean Data to PostgreSQL\n",
    "# ============================================================\n",
    "df.to_sql(\"sales_cleaned\", engine, if_exists=\"replace\", index=False)\n",
    "with engine.connect() as conn:\n",
    "    count = pd.read_sql(\"SELECT COUNT(*) as registros FROM sales_cleaned\", conn)\n",
    "    print(f\"Records in PostgreSQL: {count.iloc[0,0]:,}\")\n",
    "\n",
    "# ============================================================\n",
    "# 8. Final Summary\n",
    "# ============================================================\n",
    "print(\"\\nHU2 COMPLETED - Cleaned data ready for analysis\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
